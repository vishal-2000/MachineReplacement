{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from env import Env\n",
    "import numpy as np\n",
    "import wandb\n",
    "import copy\n",
    "from icecream import ic\n",
    "\n",
    "# wandb setup\n",
    "number = 1\n",
    "NAME = \"AC\" + str(number)\n",
    "ID = \"AC\" + str(number)\n",
    "run = wandb.init(project='actorcritic_MachineReplacement', name = NAME, id = ID)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ic(device)\n",
    "\n",
    "# %%\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_obs, n_act):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.layer = nn.Linear(n_obs, 16)\n",
    "        self.actor = nn.Linear(16, n_act)\n",
    "        self.critic = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.layer(state)\n",
    "        x = F.relu(x)\n",
    "        action_prob = F.softmax(self.actor(x), dim=-1)\n",
    "        state_vals = self.critic(x)\n",
    "\n",
    "        return action_prob, state_vals\n",
    "\n",
    "# %%\n",
    "n_obs = 1\n",
    "n_act = 2\n",
    "\n",
    "model_policy = PolicyNetwork(n_obs=n_obs, n_act=n_act).to(device)\n",
    "optimizer = optim.Adam(model_policy.parameters(), lr=3e-2)\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    steps_done += 1\n",
    "    with torch.no_grad():\n",
    "        action_probs, state_val = model_policy(state)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        action = action_dist.sample()\n",
    "        logprob = action_dist.log_prob(action)\n",
    "\n",
    "        return action, logprob, state_val\n",
    "\n",
    "# %%\n",
    "max_steps_per_episode = 500\n",
    "n_episodes = 10000\n",
    "GAMMA = 1\n",
    "R = 35 # Cost of replacement of a machine\n",
    "\n",
    "wandb.config.update({\n",
    "    'max_timesteps_per_episode': max_steps_per_episode,\n",
    "    'num_of_episodes': n_episodes,\n",
    "    'R': R,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 'default',\n",
    "    'n_actions': n_act,\n",
    "    'n_observations': n_obs,\n",
    "})\n",
    "\n",
    "env = Env(R=R)\n",
    "\n",
    "# %%\n",
    "all_rewards = []\n",
    "entropy_term = 0\n",
    "\n",
    "\n",
    "for eps in range(n_episodes):\n",
    "    logprobs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for steps in range(max_steps_per_episode):\n",
    "        state = torch.tensor([state], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        policy_dist, value = model_policy(state)\n",
    "        value = value.cpu().detach().numpy()[0, 0]\n",
    "        dist = policy_dist.cpu().detach().numpy()\n",
    "\n",
    "        action = np.random.choice(n_act, p=np.squeeze(dist))\n",
    "        logprob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        # entropy = -np.sum(np.mean(dist)*np.log(dist))\n",
    "        new_state, reward = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        logprobs.append(logprob)\n",
    "        # entropy_term += entropy\n",
    "        state = new_state\n",
    "\n",
    "        if steps == max_steps_per_episode - 1:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            _ , Qval= model_policy(state)\n",
    "            Qval = Qval.cpu().detach().numpy()[0, 0]\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA*values[t]\n",
    "        Qvals[t] = Qval\n",
    "\n",
    "    values = torch.FloatTensor(values)\n",
    "    Qvals = torch.FloatTensor(Qvals)\n",
    "    logprobs = torch.stack(logprobs)\n",
    "\n",
    "    advantage = Qvals - values\n",
    "    advantage = advantage.to(device)\n",
    "    actor_loss = (-logprobs*advantage).mean()\n",
    "    critic_loss = 0.5*advantage.pow(2).mean()\n",
    "\n",
    "    # ic(actor_loss, critic_loss, entropy_term)\n",
    "    # ac_loss = actor_loss + critic_loss + 0.001*entropy_term\n",
    "    ac_loss = actor_loss + critic_loss\n",
    "\n",
    "    wandb.log({'loss': ac_loss, 'Current_return': all_rewards[-1], 'n_episode': eps}) #, 'batch': t})\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"\\rEpisode: {eps}\\tLoss: {ac_loss}\\tCurrent Discounted Return: {all_rewards[-1]}\", end=\"\")\n",
    "\n",
    "    if eps%100 == 0:\n",
    "        SAVE_PATH = './checkpoints/AC/AC_{}.pt'.format(eps)\n",
    "        torch.save(model_policy.state_dict(), SAVE_PATH)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from env import Env\n",
    "import numpy as np\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "\n",
    "# wandb setup\n",
    "number = 1\n",
    "NAME = \"AC\" + str(number)\n",
    "ID = \"AC\" + str(number)\n",
    "run = wandb.init(project='actorcritic_MachineReplacement', name = NAME, id = ID)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ic(device)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_obs, n_act):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = n_act\n",
    "        self.critic_linear1 = nn.Linear(n_obs, 16)\n",
    "        self.critic_linear2 = nn.Linear(16, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(n_obs, 16)\n",
    "        self.actor_linear2 = nn.Linear(16, n_act)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "\n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return policy_dist, value\n",
    "\n",
    "\n",
    "# %%\n",
    "n_obs = 1\n",
    "n_act = 2\n",
    "\n",
    "model_policy = PolicyNetwork(n_obs=n_obs, n_act=n_act).to(device)\n",
    "optimizer = optim.Adam(model_policy.parameters(), lr=3e-2)\n",
    "steps_done = 0\n",
    "\n",
    "# %%\n",
    "max_steps_per_episode = 500\n",
    "n_episodes = 2000\n",
    "GAMMA = 0.001\n",
    "R = 35 # Cost of replacement of a machine\n",
    "\n",
    "wandb.config.update({\n",
    "    'max_timesteps_per_episode': max_steps_per_episode,\n",
    "    'num_of_episodes': n_episodes,\n",
    "    'R': R,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 'default',\n",
    "    'n_actions': n_act,\n",
    "    'n_observations': n_obs,\n",
    "})\n",
    "\n",
    "env = Env(R=R)\n",
    "\n",
    "# %%\n",
    "all_rewards = []\n",
    "entropy_term = 0\n",
    "\n",
    "\n",
    "for eps in range(n_episodes):\n",
    "    logprobs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for steps in range(max_steps_per_episode):\n",
    "        state = torch.tensor([state], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        policy_dist, value = model_policy(state)\n",
    "        value = value.cpu().detach().numpy()[0, 0]\n",
    "        dist = policy_dist.cpu().detach().numpy()\n",
    "\n",
    "        action = np.random.choice(n_act, p=np.squeeze(dist))\n",
    "        logprob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        # entropy = -np.sum(np.mean(dist)*np.log(dist))\n",
    "        new_state, reward = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        logprobs.append(logprob)\n",
    "        # entropy_term += entropy\n",
    "        state = new_state\n",
    "\n",
    "        if steps == max_steps_per_episode - 1:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            _, Qval = model_policy(state)\n",
    "            Qval = Qval.cpu().detach().numpy()[0, 0]\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA*Qval\n",
    "        Qvals[t] = Qval\n",
    "\n",
    "    values = torch.FloatTensor(values)\n",
    "    Qvals = torch.FloatTensor(Qvals)\n",
    "    logprobs = torch.stack(logprobs)\n",
    "\n",
    "    advantage = Qvals\n",
    "    advantage = advantage.to(device)\n",
    "    actor_loss = (-logprobs*advantage).mean()\n",
    "    critic_loss = F.smooth_l1_loss(values, Qvals)\n",
    "\n",
    "    # ic(actor_loss, critic_loss, entropy_term)\n",
    "    # ac_loss = actor_loss + critic_loss + 0.001*entropy_term\n",
    "\n",
    "    ac_loss = actor_loss + critic_loss\n",
    "\n",
    "    wandb.log({'loss': ac_loss, 'Current_return': all_rewards[-1], 'n_episode': eps}) #, 'batch': t})\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"\\rEpisode: {eps}\\tLoss: {ac_loss}\\tCurrent Discounted Return: {all_rewards[-1]}\", end=\"\")\n",
    "\n",
    "    if eps%100 == 0:\n",
    "        SAVE_PATH = './checkpoints/AC/AC_{}.pt'.format(eps)\n",
    "        torch.save(model_policy.state_dict(), SAVE_PATH)\n",
    "\n",
    "def evaluate_policy(env: Env, policy: torch.nn.Module):\n",
    "    sum_rewards = 0\n",
    "    n_episodes = 20\n",
    "    for episode_num in range(n_episodes):\n",
    "        episode_reward = 0\n",
    "        curr_state = env.reset()\n",
    "        curr_state = torch.tensor(\n",
    "            [curr_state], dtype=torch.float32, device=device\n",
    "        ).unsqueeze(0)\n",
    "        for step_num in range(max_steps_per_episode):\n",
    "            action = policy(curr_state)[0].max(1)[1].item()\n",
    "            next_state, reward = env.step(action)\n",
    "            episode_reward += reward\n",
    "            curr_state = torch.tensor(\n",
    "                [next_state], dtype=torch.float32, device=device\n",
    "            ).unsqueeze(0)\n",
    "        episode_reward /= max_steps_per_episode\n",
    "        sum_rewards += episode_reward\n",
    "    sum_rewards /= n_episodes\n",
    "    return sum_rewards\n",
    "\n",
    "def print_policy(policy: torch.nn.Module):\n",
    "    for s in range(1, 101):\n",
    "        inp = torch.tensor([s], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        print(policy(inp)[0].max(1)[1].item(), end=\" \")\n",
    "        if s % 25 == 0:\n",
    "            print()\n",
    "\n",
    "# %%\n",
    "best_reward = -torch.inf\n",
    "best_policy = PolicyNetwork(n_obs, n_act)\n",
    "for i in tqdm(range(100, n_episodes, 100), desc=\"Evaluating\", leave=False):\n",
    "    LOAD_PATH = f'./checkpoints/AC/AC_{i}.pt'\n",
    "    policy_net = PolicyNetwork(n_obs, n_act).to(device)\n",
    "    checkpoint = torch.load(LOAD_PATH)\n",
    "    policy_net.load_state_dict(checkpoint)\n",
    "    reward = evaluate_policy(env, policy_net)\n",
    "    ic(\"Reward (over policy)\", reward, i)\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_policy = deepcopy(policy_net)\n",
    "\n",
    "print_policy(best_policy)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
