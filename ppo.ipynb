{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import math\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from env import Env\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "writer = SummaryWriter(\"runs/deep-sarsa\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "ENV = Env(4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "NUM_ACTIONS = ENV.n_actions\n",
    "NUM_STEPS = int(2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state','next_action'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            #layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            layer_init(nn.Linear(1, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            #layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            layer_init(nn.Linear(1, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, NUM_ACTIONS), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent().to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LR, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((NUM_STEPS, 1) + (1,)).to(device)\n",
    "actions = torch.zeros((NUM_STEPS, 1) + (NUM_ACTIONS,)).to(device)\n",
    "logprobs = torch.zeros((NUM_STEPS, 1)).to(device)\n",
    "rewards = torch.zeros((NUM_STEPS, 1)).to(device)\n",
    "dones = torch.zeros((NUM_STEPS, 1)).to(device)\n",
    "values = torch.zeros((NUM_STEPS, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.9004e-17])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zi384s51) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-yogurt-1</strong> at: <a href='https://wandb.ai/clpsyche-team/rl%20ppo/runs/zi384s51' target=\"_blank\">https://wandb.ai/clpsyche-team/rl%20ppo/runs/zi384s51</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_161104-zi384s51/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zi384s51). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shivansh/Desktop/College/Spring_23/RL/MachineReplacement/wandb/run-20230508_161237-rrbzest9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/clpsyche-team/rl%20ppo/runs/rrbzest9' target=\"_blank\">stellar-gorge-2</a></strong> to <a href='https://wandb.ai/clpsyche-team/rl%20ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/clpsyche-team/rl%20ppo' target=\"_blank\">https://wandb.ai/clpsyche-team/rl%20ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/clpsyche-team/rl%20ppo/runs/rrbzest9' target=\"_blank\">https://wandb.ai/clpsyche-team/rl%20ppo/runs/rrbzest9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"rl ppo\",\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "writer = SummaryWriter(f\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "total_timesteps = 10000\n",
    "update_epochs = 4\n",
    "num_minibatch = 4\n",
    "minibatch_size = int(BATCH_SIZE // num_minibatch)\n",
    "next_obs = torch.Tensor(ENV.reset()).to(device)\n",
    "next_done = torch.zeros(1).to(device)\n",
    "num_updates = total_timesteps // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| next_obs: tensor([-6.9004e-17])\n",
      "ic| next_obs.shape: torch.Size([1])\n",
      "ic| next_obs: 1\n",
      "ic| next_obs: tensor([-6.9004e-17])\n",
      "ic| next_obs: tensor([-6.9004e-17])\n",
      "ic| next_obs.shape: torch.Size([1])\n",
      "ic| next_obs: 1\n",
      "ic| next_obs: tensor([-6.9004e-17])\n",
      "ic| next_obs: tensor([-6.9004e-17])\n",
      "ic| next_obs.shape: torch.Size([1])\n",
      "ic| next_obs: 1\n",
      "ic| next_obs: tensor([-3.5128e+22])\n",
      "ic| next_obs: tensor([-3.5128e+22])\n",
      "ic| next_obs.shape: torch.Size([1])\n",
      "ic| next_obs: 2\n",
      "ic| next_obs: tensor([-6.9004e-17,  4.5597e-41])\n",
      "ic| next_obs: tensor([-6.9004e-17,  4.5597e-41])\n",
      "ic| next_obs.shape: torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [1, 1].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m ic(next_obs)\n\u001b[1;32m      6\u001b[0m ic(next_obs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 7\u001b[0m obs[step] \u001b[39m=\u001b[39m next_obs\n\u001b[1;32m      8\u001b[0m dones[step] \u001b[39m=\u001b[39m next_done\n\u001b[1;32m     10\u001b[0m \u001b[39m# ALGO LOGIC: action logic\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [1, 1].  Tensor sizes: [2]"
     ]
    }
   ],
   "source": [
    "for update in range(1, num_updates + 1):\n",
    "\n",
    "    for step in range(0, NUM_STEPS):\n",
    "        global_step += 1\n",
    "        ic(next_obs)\n",
    "        ic(next_obs.shape)\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        # next_obs, reward, done, info = ENV.step(action.item())\n",
    "        next_obs, reward = ENV.step(action.item())\n",
    "        ic(next_obs)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        # next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        ic(next_obs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        returns = torch.zeros_like(rewards).to(device)\n",
    "        for t in reversed(range(NUM_STEPS)):\n",
    "            if t == NUM_STEPS - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                next_return = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                next_return = returns[t + 1]\n",
    "            returns[t] = rewards[t] + GAMMA * nextnonterminal * next_return\n",
    "        advantages = returns - values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + (1, ))\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + (NUM_ACTIONS, ))\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    b_inds = np.arange(BATCH_SIZE)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, BATCH_SIZE, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > 0.2).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "        \n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - 0.2, 1 + 0.2)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "            v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                newvalue - b_values[mb_inds],\n",
    "                -0.2,\n",
    "                0.2,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - 0.01 * entropy_loss + v_loss * 0.5\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
