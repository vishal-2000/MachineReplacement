{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvishalrm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/vishal/Desktop/acads/Intro to RL/codes/MachineReplacement/wandb/run-20230507_161620-Deep SARSA_HER1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vishalrm/DeepSARSA_HER_MachineReplacement/runs/Deep%20SARSA_HER1' target=\"_blank\">DeepSARSA_HER1</a></strong> to <a href='https://wandb.ai/vishalrm/DeepSARSA_HER_MachineReplacement' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vishalrm/DeepSARSA_HER_MachineReplacement' target=\"_blank\">https://wandb.ai/vishalrm/DeepSARSA_HER_MachineReplacement</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vishalrm/DeepSARSA_HER_MachineReplacement/runs/Deep%20SARSA_HER1' target=\"_blank\">https://wandb.ai/vishalrm/DeepSARSA_HER_MachineReplacement/runs/Deep%20SARSA_HER1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from env import Env\n",
    "import numpy as np\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "# wandb setup\n",
    "number = 1\n",
    "NAME = \"DeepSARSA_HER\" + str(number)\n",
    "ID = \"Deep SARSA_HER\" + str(number)\n",
    "run = wandb.init(project='DeepSARSA_HER_MachineReplacement', name = NAME, id = ID)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'next_action'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSARSA(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DeepSARSA, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 16)\n",
    "        self.layer2 = nn.Linear(16, 16)\n",
    "        self.layer3 = nn.Linear(16, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "env = Env(R=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "max_timesteps = 100000\n",
    "\n",
    "n_actions = env.n_actions\n",
    "state = env.reset()\n",
    "n_observations = 1\n",
    "\n",
    "policy_net = DeepSARSA(n_observations, n_actions).to(device)\n",
    "target_net = DeepSARSA(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[np.random.randint(0, 2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(timestep=0, batch_num=0, avg_reward=0):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return \n",
    "    print(\"Optimization!\")\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "    #                                      batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "    #                                            if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    next_action_batch = torch.cat(batch.next_action)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(next_state_batch).gather(1, next_action_batch) # SARSA update\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch # SARSA update\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # wandb.log({'loss': loss, 'timestep': timestep, 'batch': batch_num})\n",
    "    wandb.log({'loss': loss, 'avg_reward': avg_reward, 'timestep': timestep}) #, 'batch': t})\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    'max_timesteps': max_timesteps,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optimizer': 'Adam',\n",
    "    'learning_rate': 'default',\n",
    "    'replay_memory': REPLAY_MEMORY_SIZE, # 10000\n",
    "    'n_actions': n_actions,\n",
    "    'n_observations': n_observations\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Optimization!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishal/opt/anaconda3/envs/toy_diffusion/lib/python3.8/site-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([512, 1, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "1\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n",
      "0\n",
      "Optimization!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m prev_action \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(cur_action)\n\u001b[1;32m     22\u001b[0m prev_reward \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(reward)\n\u001b[0;32m---> 24\u001b[0m optimize_model(timestep\u001b[39m=\u001b[39;49m(i\u001b[39m*\u001b[39;49mnum_time_per_episode) \u001b[39m+\u001b[39;49m j, avg_reward\u001b[39m=\u001b[39;49mreward\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     26\u001b[0m \u001b[39m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     28\u001b[0m target_net_state_dict \u001b[39m=\u001b[39m target_net\u001b[39m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(timestep, batch_num, avg_reward)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     49\u001b[0m \u001b[39m# In-place gradient clipping\u001b[39;00m\n\u001b[1;32m     50\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(policy_net\u001b[39m.\u001b[39mparameters(), \u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/toy_diffusion/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/toy_diffusion/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "num_time_per_episode = 150\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    prev_state = 0\n",
    "    prev_action = 0\n",
    "    prev_reward = 0\n",
    "    for j in range(num_time_per_episode):\n",
    "        cur_state = env.reset()\n",
    "        cur_state = torch.tensor ([cur_state], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        cur_action = select_action(cur_state)\n",
    "        print(cur_action.item())\n",
    "        next_state, reward = env.step(cur_action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if j>0:\n",
    "            memory.push(prev_state, prev_action, prev_reward, cur_state, cur_action)\n",
    "\n",
    "        prev_state = copy.deepcopy(cur_state)\n",
    "        prev_action = copy.deepcopy(cur_action)\n",
    "        prev_reward = copy.deepcopy(reward)\n",
    "\n",
    "        optimize_model(timestep=(i*num_time_per_episode) + j, avg_reward=reward.item())\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "batch = Transition(*zip(*transitions))\n",
    "\n",
    "# Compute a mask of non-final states and concatenate the batch elements\n",
    "# (a final state would've been the one after which simulation ended)\n",
    "#non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                      batch.next_state)), device=device, dtype=torch.bool)\n",
    "#non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "#                                            if s is not None])\n",
    "# for i in range(len(batch.state)):\n",
    "    # print(batch.state[i]\n",
    "action_batch = torch.cat(batch.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "518fb82a0e41517364b8f57b2672a10bd7dfd011741d35e3037a09c2be57420f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
